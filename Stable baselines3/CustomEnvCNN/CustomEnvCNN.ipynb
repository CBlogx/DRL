{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import torch as th \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cube:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, self.size)\n",
    "        self.y = np.random.randint(0, self.size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.x},{self.y}'\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choise):\n",
    "        if choise == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choise == 1:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choise == 2:\n",
    "            self.move(x=1, y=-1)\n",
    "        elif choise == 3:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choise == 4:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choise == 5:\n",
    "            self.move(x=0, y=-1)\n",
    "        elif choise == 6:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choise == 7:\n",
    "            self.move(x=-1, y=0)\n",
    "        elif choise == 8:\n",
    "            self.move(x=-0, y=0)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x >= self.size:\n",
    "            self.x = self.size-1\n",
    "\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y >= self.size:\n",
    "            self.y = self.size-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class envCube(gym.Env):\n",
    "    SIZE = 10\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)\n",
    "    ACTION_SPACE_VALUES = 9\n",
    "    RETURN_IMAGE = True\n",
    "\n",
    "    FOOD_REWARD = 25\n",
    "    ENEMY_PENALITY = -300\n",
    "    MOVE_PENALITY = -1\n",
    "\n",
    "    d = {1: (255, 0, 0), 2: (0, 255, 0), 3: (0, 0, 255)}\n",
    "\n",
    "    PLAYER_N = 1\n",
    "    FOOD_N = 2\n",
    "    ENEMY_N = 3\n",
    "    N_CHANNELS = 3\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(envCube, self).__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(self.ACTION_SPACE_VALUES)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        # RGB is a three channel\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.N_CHANNELS, self.SIZE, self.SIZE),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.player = Cube(self.SIZE)\n",
    "        self.food = Cube(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Cube(self.SIZE)\n",
    "\n",
    "        self.enemy = Cube(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Cube(self.SIZE)\n",
    "\n",
    "        if self.RETURN_IMAGE:\n",
    "            observation = self.get_image()\n",
    "            observation = np.moveaxis(observation, -1, 0)\n",
    "        else:\n",
    "            observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "            observation = np.array(observation)\n",
    "        self.episode_step = 0\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "        self.food.move()\n",
    "        self.enemy.move()\n",
    "\n",
    "        if self.RETURN_IMAGE:\n",
    "            new_observation = self.get_image()\n",
    "            new_observation = np.moveaxis(new_observation, -1, 0)\n",
    "        else:\n",
    "            new_observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "            new_observation = np.array(new_observation)\n",
    "        if self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        elif self.player == self.enemy:\n",
    "            reward = self.ENEMY_PENALITY\n",
    "        else:\n",
    "            reward = self.MOVE_PENALITY\n",
    "\n",
    "        terminated = False\n",
    "        if (\n",
    "            self.player == self.food\n",
    "            or self.player == self.enemy\n",
    "            or self.episode_step >= 200\n",
    "        ):\n",
    "            terminated = True\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        truncated = False\n",
    "        return new_observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = Image.fromarray(img, \"RGB\")\n",
    "        img = img.resize((400, 400))\n",
    "        cv2.imshow(\"Predatoc\", np.array(img))\n",
    "        if (\n",
    "            self.player == self.food\n",
    "            or self.player == self.enemy\n",
    "            or self.episode_step >= 200\n",
    "        ):\n",
    "            cv2.waitKey(1500)\n",
    "        else:\n",
    "            cv2.waitKey(50)\n",
    "\n",
    "    def get_image(self):\n",
    "        img = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)\n",
    "        img[self.food.x][self.food.y] = self.d[self.FOOD_N]\n",
    "        img[self.player.x][self.player.y] = self.d[self.PLAYER_N]\n",
    "        img[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Download\\Anaconda\\envs\\py3.9\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:51: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = envCube()\n",
    "print(env.observation_space.shape)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 8, kernel_size=(3, 3), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8, kernel_size=(3, 3), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8, kernel_size=(3, 3), stride=1, padding=0),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    "    # net_arch=[dict(vf=[16], pi=[32])],\n",
    "    activation_fn = th.nn.ReLU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = A2C(\"CnnPolicy\", env, verbose=1)\n",
    "model = A2C(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"./tb_logs\",\n",
    "    learning_rate=1e-3,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): CustomCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): CustomCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): CustomCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=9, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最优模型\n",
    "eval_callback = EvalCallback(env, best_model_save_path=\"./tb_logs/bestModel\",\n",
    "                             log_path=\"./tb_logs/bestModel\", eval_freq=500,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['timesteps', 'results', 'ep_lengths']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载日志信息\n",
    "logfile = np.load('./tb_logs/bestModel/evaluations.npz')\n",
    "logfile.files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-200.,  -51., -101., -319.,  -16., -433., -200., -304., -422.,\n",
       "       -340.,  -67., -200.,  -65., -302., -200., -200., -368., -498.,\n",
       "       -345.,  -87.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfile['timesteps'].shape\n",
    "logfile['results'].shape\n",
    "logfile['ep_lengths'].shape\n",
    "logfile['results'][:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=int(1e4),\n",
    "    progress_bar=True,\n",
    "    tb_log_name=\"Custom_CNN_A2C_Net128x16x32_1W_call\",\n",
    "    callback= eval_callback\n",
    ")\n",
    "model.save(\"Custom_CNN_A2C_Net128x16x32_1W_call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = A2C.load(\"Custom_CNN_A2C_Net128x16x32_2M\",env=env)\n",
    "# mean_reward, std_reward = evaluate_policy(\n",
    "#     model, model.get_env(), deterministic=False, render=False, n_eval_episodes=10\n",
    "# )\n",
    "# print(mean_reward, std_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # env测试代码\n",
    "# eposides = 10\n",
    "# for ep in range(eposides):\n",
    "#     # 初始化\n",
    "#     obs, _ = env.reset()\n",
    "#     terminated = False\n",
    "#     rewards = 0\n",
    "#     while not terminated:\n",
    "#         # action = env.action_space.sample()\n",
    "#         action, _states = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, terminated, info, _ = env.step(action)\n",
    "#         env.render()\n",
    "#         rewards += reward\n",
    "#     print(terminated,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-py3.9",
   "language": "python",
   "name": "jupyter-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
